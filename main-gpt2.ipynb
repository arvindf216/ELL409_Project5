{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"V28"},"accelerator":"TPU","kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":9907308,"sourceType":"datasetVersion","datasetId":6086882},{"sourceId":9998067,"sourceType":"datasetVersion","datasetId":6153746}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom transformers import PreTrainedTokenizerFast, AutoTokenizer\nfrom torch.optim import AdamW\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nimport math\n\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"id":"EqlBgw5kMiiv","outputId":"c403120f-ab17-4340-cb2e-7b0a43e632c7","execution":{"iopub.status.busy":"2024-11-24T09:22:56.585694Z","iopub.execute_input":"2024-11-24T09:22:56.586105Z","iopub.status.idle":"2024-11-24T09:23:27.944605Z","shell.execute_reply.started":"2024-11-24T09:22:56.586077Z","shell.execute_reply":"2024-11-24T09:23:27.943570Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/usr/local/lib/python3.10/site-packages/torch_xla/__init__.py:202: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Load and preprocess the text data\nwith open(\"/kaggle/input/frost-poems/frost_poems.txt\", \"r\") as f:\n    corpus = f.readlines()\n\ntext = \" \".join(line.strip() for line in corpus if line.strip())\ntext = text.replace('.','').replace(',','')","metadata":{"id":"hv9IsIGEMpH2","execution":{"iopub.status.busy":"2024-11-24T09:23:27.946157Z","iopub.execute_input":"2024-11-24T09:23:27.946593Z","iopub.status.idle":"2024-11-24T09:23:27.959816Z","shell.execute_reply.started":"2024-11-24T09:23:27.946563Z","shell.execute_reply":"2024-11-24T09:23:27.958959Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Initialize tokenizer and model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\nmodel.train()","metadata":{"id":"ugXFRaLBMoMl","execution":{"iopub.status.busy":"2024-11-24T09:23:27.960774Z","iopub.execute_input":"2024-11-24T09:23:27.961038Z","iopub.status.idle":"2024-11-24T09:23:33.401597Z","shell.execute_reply.started":"2024-11-24T09:23:27.961011Z","shell.execute_reply":"2024-11-24T09:23:33.400805Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D(nf=2304, nx=768)\n          (c_proj): Conv1D(nf=768, nx=768)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D(nf=3072, nx=768)\n          (c_proj): Conv1D(nf=768, nx=3072)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Tokenize and chunk text\nmax_len = 50\ntokens = tokenizer.tokenize(text)\ninput_ids = [tokenizer.convert_tokens_to_ids(tokens[i:i + max_len]) for i in range(0, len(tokens), max_len)]\ninput_ids = [torch.tensor(ids) for ids in input_ids]","metadata":{"id":"CdZzyk6cM1Rw","execution":{"iopub.status.busy":"2024-11-24T09:23:33.403434Z","iopub.execute_input":"2024-11-24T09:23:33.403728Z","iopub.status.idle":"2024-11-24T09:23:33.612908Z","shell.execute_reply.started":"2024-11-24T09:23:33.403700Z","shell.execute_reply":"2024-11-24T09:23:33.612069Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Custom Dataset Class\nclass PoemDataset(Dataset):\n    def __init__(self, input_ids, max_len=512):\n        self.input_ids = input_ids\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.input_ids) - 1\n\n    def __getitem__(self, idx):\n        input_ids = self.input_ids[idx]\n        labels = self.input_ids[idx + 1]\n\n        # Pad to max_len\n        input_ids = torch.cat([input_ids, torch.zeros(self.max_len - len(input_ids), dtype=torch.long)])\n        labels = torch.cat([labels, torch.zeros(self.max_len - len(labels), dtype=torch.long)])\n\n        return input_ids, labels","metadata":{"id":"0H3Q76GZM2zF","execution":{"iopub.status.busy":"2024-11-24T09:23:33.613962Z","iopub.execute_input":"2024-11-24T09:23:33.614262Z","iopub.status.idle":"2024-11-24T09:23:35.492177Z","shell.execute_reply.started":"2024-11-24T09:23:33.614236Z","shell.execute_reply":"2024-11-24T09:23:35.491302Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Dataset and DataLoader\ndataset = PoemDataset(input_ids)\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)","metadata":{"id":"BF7gN8MlM3iu","execution":{"iopub.status.busy":"2024-11-24T09:23:35.493217Z","iopub.execute_input":"2024-11-24T09:23:35.493473Z","iopub.status.idle":"2024-11-24T09:23:35.606300Z","shell.execute_reply.started":"2024-11-24T09:23:35.493449Z","shell.execute_reply":"2024-11-24T09:23:35.605551Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Define optimizer\noptimizer = AdamW(model.parameters(), lr=1e-3)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"id":"IbyUc6YjM5Dm","execution":{"iopub.status.busy":"2024-11-24T09:23:35.607300Z","iopub.execute_input":"2024-11-24T09:23:35.607618Z","iopub.status.idle":"2024-11-24T09:23:35.773298Z","shell.execute_reply.started":"2024-11-24T09:23:35.607590Z","shell.execute_reply":"2024-11-24T09:23:35.772551Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Move model to the selected device\nmodel = model.to(device)\n\n# Training loop\nnum_epochs = 10\nprint(\"Starting: \")\nfor epoch in range(num_epochs):\n    for input_ids, labels in train_loader:\n        # Move input_ids and labels to the same device as the model\n        input_ids, labels = input_ids.to(device), labels.to(device)\n        optimizer.zero_grad()\n        # Forward pass\n        outputs = model(input_ids=input_ids, labels=labels)\n        loss = outputs.loss\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")","metadata":{"id":"ucHX9JEsM-Nd","execution":{"iopub.status.busy":"2024-11-24T09:23:35.774215Z","iopub.execute_input":"2024-11-24T09:23:35.774453Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Starting: \nEpoch 1, Loss: 0.6258116364479065\nEpoch 2, Loss: 0.6213662028312683\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Evaluation\nmodel.eval()\n\ndef compute_perplexity(log_probs):\n    return math.exp(-np.mean(log_probs))","metadata":{"id":"e7Ruux6NM-0A","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_labels, all_preds, log_probs = [], [], []\n\nwith torch.no_grad():\n    for input_ids, labels in test_loader:\n        input_ids, labels = input_ids.to(device), labels.to(device)\n        outputs = model(input_ids=input_ids, labels=labels)\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=-1)\n\n        # Calculate log probabilities\n        log_probs_batch = torch.gather(logits, 2, labels.unsqueeze(-1)).squeeze().cpu().tolist()\n        log_probs.extend(log_probs_batch)\n\n        all_labels.extend(labels.flatten().tolist())\n        all_preds.extend(predictions.flatten().tolist())\n\n\naccuracy = accuracy_score(all_labels, all_preds)\nperplexity = compute_perplexity(log_probs)\n\nprint(f\"Test Accuracy: {accuracy}\")\nprint(f\"Test Perplexity: {perplexity}\")","metadata":{"id":"-ChvlDqUNGH3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Text Generation Function\ndef generate_text(model, tokenizer, seed_text, next_words=20, max_len=512, num_beams=5):\n    model.eval()\n    \n    # Set pad token ID if it is not already set\n    if tokenizer.pad_token_id is None:\n        tokenizer.pad_token_id = tokenizer.eos_token_id or 0  # Defaults to 0 if EOS token is also None\n    \n    input_ids = tokenizer.encode(seed_text, return_tensors=\"pt\", max_length=max_len, truncation=True)\n    \n    # Add attention mask to prevent unexpected behavior with padding\n    attention_mask = (input_ids != tokenizer.pad_token_id).long()\n\n    with torch.no_grad():\n        output = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_length=max_len + next_words,\n            num_beams=num_beams,  # Enables beam search\n            no_repeat_ngram_size=2,\n            early_stopping=True,\n            pad_token_id=tokenizer.pad_token_id\n        )\n\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n    return generated_text\n\n# Generate example text\nprint(generate_text(model, tokenizer, \"I wish\", next_words=20))","metadata":{"id":"rAIgMkzPNIoP","trusted":true},"outputs":[],"execution_count":null}]}